{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDAN20 - Assign 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency parsing using machine learning techniques\n",
    "This is the lab notebook for the EDAN20: Language Technology course offered in the HT1 2019 period at Lunds Tekniska HÃ¶gskola (LTH).\n",
    "\n",
    "This assignment was finished by Jonathan Moran (jo6155mo-s) and Alexis Cole (). The initial code, assignment files and instructions were prepared by Pierre Nugues(@pnugues/ilppp).\n",
    "\n",
    "More information is available at the kurswebb here: http://cs.lth.se/edan20/coursework/assignment-6/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "The objectives of this assignment are to:\n",
    "- Extract feature vectors and train a classifier\n",
    "- Write a statistical dependency parser\n",
    "- Understand how to design parameter sets\n",
    "- Write a short report on your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session, you will implement and test a dependency parser for Swedish using machine learning techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a training and a test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_file = str(train_data)\n",
    "train_file = 'swedish_talbanken05_train.conll'\n",
    "\n",
    "\n",
    "# test_data = urlopen('http://fileadmin.cs.lth.se/cs/Education/EDAN20/corpus/conllx/sv/swedish_talbanken05_test_blind.conll').read()\n",
    "test_file = 'swedish_talbanken05_test.conll'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the classifiers\n",
    "If you have not done it in the previous assignment, for each data set you have generated, fit a corresponding model using logistic regression (or another classifier if you want) and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dparser\n",
    "import conll\n",
    "import transition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import transition\n",
    "\n",
    "def extract(stack, queue, graph, feature_names, sentence):\n",
    "    \n",
    "    \"\"\"\n",
    "    Use three feature sets to train the specified classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    features = [] \n",
    "    \"\"\"\n",
    "    features1 -- first element on stack/queue\n",
    "    \"\"\"\n",
    "    # given first 4 parameters (x)\n",
    "    features.extend(['nil', 'nil', 'nil', 'nil', 'nil', 'nil'])\n",
    "    # label the first word + POS extracted from the stack\n",
    "    if len(stack) >= 1:\n",
    "        features[0] = stack[0]['form']\n",
    "        features[1] = stack[0]['postag']\n",
    "    # label the first word + POS extracted from the queue\n",
    "    if len(queue) >= 1:\n",
    "        features[2] = queue[0]['form']\n",
    "        features[3] = queue[0]['postag']\n",
    "    # append the two Boolean parameters\n",
    "    features[4] = transition.can_leftarc(stack, graph)\n",
    "    features[5] = transition.can_reduce(stack, graph)\n",
    "    \n",
    "    \"\"\"\n",
    "    features2 -- first and second element on stack/queue\n",
    "    \"\"\"\n",
    "    # check if expected feature set (num of params)\n",
    "    if len(feature_names) == 10 or len(feature_names) == 14:\n",
    "        features.extend(['nil', 'nil', 'nil', 'nil'])\n",
    "        # more than one element on stack\n",
    "        if len(stack) >= 2:\n",
    "            features[6] = stack[1]['form']\n",
    "            features[7] = stack[1]['postag']\n",
    "        # more than one element on queue\n",
    "        if len(queue) >= 2:\n",
    "            features[8] = queue[1]['form']\n",
    "            features[9] = queue[1]['postag']\n",
    "        \n",
    "    \"\"\"\n",
    "    features3 -- first element + 2 addtl (one must be prev. element)\n",
    "    \"\"\"\n",
    "    if len(feature_names) == 14:\n",
    "        features.extend(['nil', 'nil', 'nil', 'nil'])\n",
    "        if len(stack) >= 1 and len(sentence) > int(stack[0]['id']) +1:\n",
    "            word = sentence[int(stack[0]['id']) +1]\n",
    "            features[10] = word['form']\n",
    "            features[11] = word['postag']\n",
    "        \n",
    "        if len(queue) >= 1 and len(sentence) > int(queue[0]['id']) +1:\n",
    "            word = sentence[int(stack[0]['id']) +1]\n",
    "            features[12] = word['form']\n",
    "            features[13] = word['postag']\n",
    "    \n",
    "    #features = dict(zip(feature_names, features))\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extract_features(sentences, feature_names):\n",
    "    \"\"\"\n",
    "    _Similar to Lab 3, Task 4: Improving the Chunker_\n",
    "    Builds X matrix and y vector\n",
    "    X is a list of dicitionaries and y is a list\n",
    "    :param sentences:\n",
    "    :param feature_names:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    X_l = []\n",
    "    y_l = []\n",
    "    sent_cnt = 0\n",
    "    \n",
    "    for i, sent in enumerate(sentences):\n",
    "        \"\"\"\n",
    "        Forming initial model structures -- dparser.py\n",
    "        \"\"\"\n",
    "        graph = {}\n",
    "        graph['heads'] = {}\n",
    "        graph['heads']['0'] = '0'\n",
    "        graph['deprels'] = {}\n",
    "        graph['deprels']['0'] = 'ROOT'\n",
    "        stack = []\n",
    "        queue = list(sent)\n",
    "        \n",
    "        while queue:\n",
    "            x = extract(stack, queue, graph, feature_names, sent)\n",
    "            X_l.append(x)\n",
    "            stack, queue, graph, trans = dparser.reference(stack, queue, graph)\n",
    "            y_l.append(trans)\n",
    "        \n",
    "        stack, graph = transition.empty_stack(stack, graph)\n",
    "        sent_cnt += 1\n",
    "        \n",
    "    return X_l, y_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def extract(stack, queue, graph, feature_names, sentence):\\n    \\n    \"\"\"\\n    Use three feature sets to train the specified classifier\\n    \"\"\"\\n    \\n    features = [] \\n    \"\"\"\\n    features1 -- first element on stack/queue\\n    \"\"\"\\n    print(feature_names)\\n    # given first 4 parameters (x)\\n    if len(feature_names) == 4 or len(feature_names) == 6:\\n        features = [\\'nil\\', \\'nil\\', \\'nil\\', \\'nil\\', \\'nil\\', \\'nil\\']\\n        # label the first word + POS extracted from the stack\\n        if len(stack) >= 1:\\n            features[0] = stack[0][\\'postag\\']\\n            features[1] = stack[0][\\'form\\']\\n        # label the first word + POS extracted from the queue\\n        if len(queue) >= 1:\\n            features[2] = queue[0][\\'postag\\']\\n            features[3] = queue[0][\\'form\\']\\n        # append the two Boolean parameters\\n        features[4] = transition.can_leftarc(stack, graph)\\n        features[5] = transition.can_reduce(stack, graph)\\n        feature_names.extend([\\'canLa\\',\\'canRe\\'])\\n        #features = dict(zip(feature_names, features))\\n    \\n        return features\\n    \\n    \"\"\"\\n    features2 -- first and second element on stack/queue\\n    \"\"\"\\n    # check if expected feature set (num of params)\\n    if len(feature_names) == 8 or len(feature_names) == 10:\\n        features = [\\'nil\\', \\'nil\\', \\'nil\\', \\'nil\\',\\'nil\\', \\'nil\\', \\'nil\\', \\'nil\\', \\'nil\\', \\'nil\\']\\n        # more than one element on stack\\n        if len(stack) >= 2:\\n            features[0] = stack[0][\\'postag\\']\\n            features[1] = stack[1][\\'postag\\']\\n            features[2] = stack[0][\\'form\\']\\n            features[3] = stack[1][\\'form\\']\\n        # more than one element on queue\\n        if len(queue) >= 2:\\n            features[4] = queue[0][\\'postag\\']\\n            features[5] = queue[1][\\'postag\\']\\n            features[6] = queue[0][\\'form\\']\\n            features[7] = queue[1][\\'form\\']\\n            \\n        features[8] = transition.can_leftarc(stack, graph)\\n        features[9] = transition.can_reduce(stack, graph)\\n        feature_names.extend([\\'canLa\\',\\'canRe\\'])\\n        #features = dict(zip(feature_names, features))\\n    \\n        return features\\n        \\n    \"\"\"\\n    features3 -- first element + 2 addtl (one must be prev. element)\\n    \"\"\"\\n    if len(feature_names) == 12 or len(feature_names) == 14:\\n        features = [\\'nil\\', \\'nil\\', \\'nil\\', \\'nil\\',\\'nil\\', \\'nil\\', \\'nil\\', \\'nil\\',\\'nil\\', \\'nil\\', \\'nil\\', \\'nil\\', \\'nil\\', \\'nil\\']\\n        if len(stack) > 2:\\n            features[0] = stack[0][\\'postag\\']\\n            features[1] = stack[1][\\'postag\\']\\n            features[2] = stack[2][\\'postag\\']\\n            features[3] = stack[0][\\'form\\']\\n            features[4] = stack[1][\\'form\\']\\n            features[5] = stack[2][\\'form\\']\\n        \\n        if len(queue) > 2:\\n            features[6] = queue[0][\\'postag\\']\\n            features[7] = queue[1][\\'postag\\']\\n            features[8] = queue[2][\\'postag\\']\\n            features[9] = queue[0][\\'form\\']\\n            features[10] = queue[1][\\'form\\']\\n            features[11] = queue[2][\\'form\\']\\n            \\n        features[12] = transition.can_leftarc(stack, graph)\\n        features[13] = transition.can_reduce(stack, graph)\\n        feature_names.extend([\\'canLa\\',\\'canRe\\'])\\n    \\n    #features = dict(zip(feature_names, features))\\n    \\n    return features\\n\\n\\ndef extract_features(sentences, feature_names):\\n    \"\"\"\\n    _Similar to Lab 3, Task 4: Improving the Chunker_\\n    Builds X matrix and y vector\\n    X is a list of dicitionaries and y is a list\\n    :param sentences:\\n    :param feature_names:\\n    :return:\\n    \"\"\"\\n    \\n    X_l = []\\n    y_l = []\\n    sent_cnt = 0\\n    \\n    for i, sent in enumerate(sentences):\\n        \"\"\"\\n        Forming initial model structures -- dparser.py\\n        \"\"\"\\n        graph = {}\\n        graph[\\'heads\\'] = {}\\n        graph[\\'heads\\'][\\'0\\'] = \\'0\\'\\n        graph[\\'deprels\\'] = {}\\n        graph[\\'deprels\\'][\\'0\\'] = \\'ROOT\\'\\n        stack = []\\n        queue = list(sent)\\n        \\n        while queue:\\n            x = extract(stack, queue, graph, feature_names, sent)\\n            X_l.append(x)\\n            stack, queue, graph, trans = dparser.reference(stack, queue, graph)\\n            y_l.append(trans)\\n        \\n        stack, graph = transition.empty_stack(stack, graph)\\n        sent_cnt += 1\\n        \\n    return X_l, y_l'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def extract(stack, queue, graph, feature_names, sentence):\n",
    "    \n",
    "    \"\"\"\n",
    "    Use three feature sets to train the specified classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    features = [] \n",
    "    \"\"\"\n",
    "    features1 -- first element on stack/queue\n",
    "    \"\"\"\n",
    "    print(feature_names)\n",
    "    # given first 4 parameters (x)\n",
    "    if len(feature_names) == 4 or len(feature_names) == 6:\n",
    "        features = ['nil', 'nil', 'nil', 'nil', 'nil', 'nil']\n",
    "        # label the first word + POS extracted from the stack\n",
    "        if len(stack) >= 1:\n",
    "            features[0] = stack[0]['postag']\n",
    "            features[1] = stack[0]['form']\n",
    "        # label the first word + POS extracted from the queue\n",
    "        if len(queue) >= 1:\n",
    "            features[2] = queue[0]['postag']\n",
    "            features[3] = queue[0]['form']\n",
    "        # append the two Boolean parameters\n",
    "        features[4] = transition.can_leftarc(stack, graph)\n",
    "        features[5] = transition.can_reduce(stack, graph)\n",
    "        feature_names.extend(['canLa','canRe'])\n",
    "        #features = dict(zip(feature_names, features))\n",
    "    \n",
    "        return features\n",
    "    \n",
    "    \"\"\"\n",
    "    features2 -- first and second element on stack/queue\n",
    "    \"\"\"\n",
    "    # check if expected feature set (num of params)\n",
    "    if len(feature_names) == 8 or len(feature_names) == 10:\n",
    "        features = ['nil', 'nil', 'nil', 'nil','nil', 'nil', 'nil', 'nil', 'nil', 'nil']\n",
    "        # more than one element on stack\n",
    "        if len(stack) >= 2:\n",
    "            features[0] = stack[0]['postag']\n",
    "            features[1] = stack[1]['postag']\n",
    "            features[2] = stack[0]['form']\n",
    "            features[3] = stack[1]['form']\n",
    "        # more than one element on queue\n",
    "        if len(queue) >= 2:\n",
    "            features[4] = queue[0]['postag']\n",
    "            features[5] = queue[1]['postag']\n",
    "            features[6] = queue[0]['form']\n",
    "            features[7] = queue[1]['form']\n",
    "            \n",
    "        features[8] = transition.can_leftarc(stack, graph)\n",
    "        features[9] = transition.can_reduce(stack, graph)\n",
    "        feature_names.extend(['canLa','canRe'])\n",
    "        #features = dict(zip(feature_names, features))\n",
    "    \n",
    "        return features\n",
    "        \n",
    "    \"\"\"\n",
    "    features3 -- first element + 2 addtl (one must be prev. element)\n",
    "    \"\"\"\n",
    "    if len(feature_names) == 12 or len(feature_names) == 14:\n",
    "        features = ['nil', 'nil', 'nil', 'nil','nil', 'nil', 'nil', 'nil','nil', 'nil', 'nil', 'nil', 'nil', 'nil']\n",
    "        if len(stack) > 2:\n",
    "            features[0] = stack[0]['postag']\n",
    "            features[1] = stack[1]['postag']\n",
    "            features[2] = stack[2]['postag']\n",
    "            features[3] = stack[0]['form']\n",
    "            features[4] = stack[1]['form']\n",
    "            features[5] = stack[2]['form']\n",
    "        \n",
    "        if len(queue) > 2:\n",
    "            features[6] = queue[0]['postag']\n",
    "            features[7] = queue[1]['postag']\n",
    "            features[8] = queue[2]['postag']\n",
    "            features[9] = queue[0]['form']\n",
    "            features[10] = queue[1]['form']\n",
    "            features[11] = queue[2]['form']\n",
    "            \n",
    "        features[12] = transition.can_leftarc(stack, graph)\n",
    "        features[13] = transition.can_reduce(stack, graph)\n",
    "        feature_names.extend(['canLa','canRe'])\n",
    "    \n",
    "    #features = dict(zip(feature_names, features))\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_features(sentences, feature_names):\n",
    "    \"\"\"\n",
    "    _Similar to Lab 3, Task 4: Improving the Chunker_\n",
    "    Builds X matrix and y vector\n",
    "    X is a list of dicitionaries and y is a list\n",
    "    :param sentences:\n",
    "    :param feature_names:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    X_l = []\n",
    "    y_l = []\n",
    "    sent_cnt = 0\n",
    "    \n",
    "    for i, sent in enumerate(sentences):\n",
    "        \"\"\"\n",
    "        Forming initial model structures -- dparser.py\n",
    "        \"\"\"\n",
    "        graph = {}\n",
    "        graph['heads'] = {}\n",
    "        graph['heads']['0'] = '0'\n",
    "        graph['deprels'] = {}\n",
    "        graph['deprels']['0'] = 'ROOT'\n",
    "        stack = []\n",
    "        queue = list(sent)\n",
    "        \n",
    "        while queue:\n",
    "            x = extract(stack, queue, graph, feature_names, sent)\n",
    "            X_l.append(x)\n",
    "            stack, queue, graph, trans = dparser.reference(stack, queue, graph)\n",
    "            y_l.append(trans)\n",
    "        \n",
    "        stack, graph = transition.empty_stack(stack, graph)\n",
    "        sent_cnt += 1\n",
    "        \n",
    "    return X_l, y_l'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(stack, queue, graph, feature_names, sentence):\n",
    "    \n",
    "    \"\"\"\n",
    "    Use three feature sets to train the specified classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    features = [] \n",
    "    \n",
    "    for f in feature_names:\n",
    "        if f == 'stack_pos_0':\n",
    "            if stack:\n",
    "                features.append(stack[0][\"postag\"])\n",
    "            else:\n",
    "                features.append(\"nil\")\n",
    "        if f == 'stack_pos_1':\n",
    "            if len(stack) > 1:\n",
    "                features.append(stack[1][\"postag\"])\n",
    "            else:\n",
    "                features.append(\"nil\")\n",
    "        if f == 'stack_pos_2':\n",
    "            if len(stack) > 2:\n",
    "                features.append(stack[2][\"postag\"])\n",
    "            else:\n",
    "                features.append(\"nil\")\n",
    "        if f == 'stack_word_0':\n",
    "            if stack:\n",
    "                features.append(stack[0][\"form\"])\n",
    "            else:\n",
    "                features.append(\"nil\")\n",
    "        if f == 'stack_word_1':\n",
    "            if len(stack) > 1:\n",
    "                features.append(stack[1][\"form\"])\n",
    "            else:\n",
    "                features.append(\"nil\")\n",
    "        if f == 'stack_word_2':\n",
    "            if len(stack) > 2:\n",
    "                features.append(stack[2][\"form\"])\n",
    "            else:\n",
    "                features.append(\"nil\")\n",
    "        if f == 'queue_pos_0':\n",
    "            if queue:\n",
    "                features.append(queue[0][\"postag\"])\n",
    "            else:\n",
    "                features.append(\"nil\")\n",
    "        if f == 'queue_pos_1':\n",
    "            if len(queue) > 1:\n",
    "                features.append(queue[1][\"postag\"])\n",
    "            else:\n",
    "                features.append(\"nil\")\n",
    "        if f == 'queue_pos_2':\n",
    "            if len(queue) > 2:\n",
    "                features.append(queue[2][\"postag\"])\n",
    "            else:\n",
    "                features.append(\"nil\")\n",
    "        if f == 'queue_word_0':\n",
    "            if queue:\n",
    "                features.append(queue[0][\"form\"])\n",
    "            else:\n",
    "                features.append(\"nil\")\n",
    "        if f == 'queue_word_1':\n",
    "            if len(queue) > 1:\n",
    "                features.append(queue[1][\"form\"])\n",
    "            else:\n",
    "                features.append(\"nil\")\n",
    "        if f == 'queue_word_2':\n",
    "            if len(queue) > 2:\n",
    "                features.append(queue[2][\"form\"])\n",
    "            else:\n",
    "                features.append(\"nil\")\n",
    "                \n",
    "    features.append(str(transition.can_reduce(stack, graph)))\n",
    "    features.append(str(transition.can_leftarc(stack, graph)))\n",
    "    #feature_names.append('canRe')\n",
    "    #feature_names.append('canLa')\n",
    "    features = dict(zip(feature_names, features))\n",
    "    return features\n",
    "        \n",
    "        \n",
    "        \n",
    "def extract_features(sentences, feature_names):\n",
    "    \"\"\"\n",
    "    _Similar to Lab 3, Task 4: Improving the Chunker_\n",
    "    Builds X matrix and y vector\n",
    "    X is a list of dicitionaries and y is a list\n",
    "    :param sentences:\n",
    "    :param feature_names:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    X_l = []\n",
    "    y_l = []\n",
    "    sent_cnt = 0\n",
    "    \n",
    "    for i, sent in enumerate(sentences):\n",
    "        \"\"\"\n",
    "        Forming initial model structures -- dparser.py\n",
    "        \"\"\"\n",
    "        graph = {}\n",
    "        graph['heads'] = {}\n",
    "        graph['heads']['0'] = '0'\n",
    "        graph['deprels'] = {}\n",
    "        graph['deprels']['0'] = 'ROOT'\n",
    "        stack = []\n",
    "        queue = list(sent)\n",
    "        \n",
    "        while queue:\n",
    "            x = extract(stack, queue, graph, feature_names, sent)\n",
    "            X_l.append(x)\n",
    "            stack, queue, graph, trans = dparser.reference(stack, queue, graph)\n",
    "            y_l.append(trans)\n",
    "        \n",
    "        stack, graph = transition.empty_stack(stack, graph)\n",
    "        sent_cnt += 1\n",
    "        for word in sent:\n",
    "            word['head'] = graph['heads'][word['id']]\n",
    "        \n",
    "    return X_l, y_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names_2006 = ['id', 'form', 'lemma', 'cpostag', 'postag', 'feats', 'head', 'deprel', 'phead', 'pdeprel']\n",
    "# column_names_2006_test = ['id', 'form', 'lemma', 'cpostag', 'postag', 'feats']\n",
    "\n",
    "sentences = conll.read_sentences(\"swedish_talbanken05_train.conll\")\n",
    "formatted_corpus = conll.split_rows(sentences, column_names_2006)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "From Lab 5 Task 4,1\n",
    "You will consider three feature sets:\n",
    "\"\"\"\n",
    "#   1) features1: word + pos\n",
    "features1 = ['stack_pos_0', 'stack_word_0', 'queue_pos_0', 'queue_word_0']\n",
    "\n",
    "#   2) features2: word, pos + prev element\n",
    "features2 = ['stack_pos_0', 'stack_pos_1', 'stack_word_0', 'stack_word_1',\n",
    "             'queue_pos_0', 'queue_pos_0', 'queue_word_0', 'queue_word_1']\n",
    "\n",
    "#   3) first/second word + pos, next word + pos:\n",
    "features3 = ['stack_pos_0', 'stack_pos_1', 'stack_pos_2', 'stack_word_0', \n",
    "            'stack_word_1', 'stack_word_2', 'queue_pos_0', 'queue_pos_1', \n",
    "             'queue_pos_2', 'queue_word_0', 'queue_word_1', 'queue_word_2']\n",
    "\n",
    "\"\"\"\n",
    "From Lab 5 Task 4,1\n",
    "You will consider three feature sets:\n",
    "\"\"\"\n",
    "#   1) features1: word + pos\n",
    "train_features1 = ['stack_pos_0', 'stack_word_0', 'queue_pos_0', 'queue_word_0', 'canRe', 'canLa']\n",
    "\n",
    "#   2) features2: word, pos + prev element\n",
    "train_features2 = ['stack_pos_0', 'stack_pos_1', 'stack_word_0', 'stack_word_1',\n",
    "                 'queue_pos_0', 'queue_pos_0', 'queue_word_0', 'queue_word_1',\n",
    "                'canRe', 'canLa']\n",
    "\n",
    "#   3) first/second word + pos, next word + pos:\n",
    "train_features3 = ['stack_pos_0', 'stack_pos_1', 'stack_pos_2', 'stack_word_0', \n",
    "                'stack_word_1', 'stack_word_2', 'queue_pos_0', 'queue_pos_1', \n",
    "                'queue_pos_2', 'queue_word_0', 'queue_word_1', 'queue_word_2', \n",
    "                 'canRe', 'canLa']\n",
    "\n",
    "X_1, y_1 = extract_features(formatted_corpus, train_features1)\n",
    "X_2, y_2 = extract_features(formatted_corpus, train_features2)\n",
    "X_3, y_3 = extract_features(formatted_corpus, train_features3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stack_pos_0': 'ROOT', 'stack_word_0': 'ROOT', 'queue_pos_0': 'NN', 'queue_word_0': 'Ãktenskapet', 'canRe': 'True', 'canLa': 'False'}\n",
      "{'stack_pos_0': 'ROOT', 'stack_pos_1': 'nil', 'stack_word_0': 'ROOT', 'stack_word_1': 'nil', 'queue_pos_0': 'NN', 'queue_word_0': 'Ãktenskapet', 'queue_word_1': 'och', 'canRe': 'True', 'canLa': 'False'}\n",
      "{'stack_pos_0': 'ROOT', 'stack_pos_1': 'nil', 'stack_pos_2': 'nil', 'stack_word_0': 'ROOT', 'stack_word_1': 'nil', 'stack_word_2': 'nil', 'queue_pos_0': 'NN', 'queue_pos_1': '++', 'queue_pos_2': 'NN', 'queue_word_0': 'Ãktenskapet', 'queue_word_1': 'och', 'queue_word_2': 'familjen', 'canRe': 'True', 'canLa': 'False'}\n"
     ]
    }
   ],
   "source": [
    "print(X_1[1])\n",
    "print(X_2[1])\n",
    "print(X_3[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the corpus and evaluating the results\n",
    "Once you have generated your models, you will embed them in Nivre's parser and compute their respective efficiencies.\n",
    "\n",
    "Your parser will proceed, sentence by sentence, and word by word. For a certain state, it will predict the next action using your classifier. You will then execute the corresponding action: `la`, `ra`, `re`, or `sh`. If an action is not possible, you will carry out a `shift`.\n",
    "\n",
    "You are free to implement it the way you want. Here are some suggestions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **The loop will basically have this structure:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "while queue:\n",
    "    features.extract()\n",
    "    trans_nr = classifier.predict()\n",
    "    stack, queue, graph, trans = parse_ml(stack, queue, graph, trans)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "From Lab 5 Task 4,1\n",
    "You will consider three feature sets:\n",
    "\"\"\"\n",
    "#   1) features1: word + pos\n",
    "train_features1 = ['stack_pos_0', 'stack_word_0', 'queue_pos_0', 'queue_word_0', 'canRe', 'canLa']\n",
    "\n",
    "#   2) features2: word, pos + prev element\n",
    "train_features2 = ['stack_pos_0', 'stack_pos_1', 'stack_word_0', 'stack_word_1',\n",
    "                 'queue_pos_0', 'queue_pos_0', 'queue_word_0', 'queue_word_1',\n",
    "                'canRe', 'canLa']\n",
    "\n",
    "#   3) first/second word + pos, next word + pos:\n",
    "train_features3 = ['stack_pos_0', 'stack_pos_1', 'stack_pos_2', 'stack_word_0', \n",
    "                'stack_word_1', 'stack_word_2', 'queue_pos_0', 'queue_pos_1', \n",
    "                'queue_pos_2', 'queue_word_0', 'queue_word_1', 'queue_word_2', \n",
    "                 'canRe', 'canLa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_dict(X_sentences, feature_names):\n",
    "    X_train = []\n",
    "    \n",
    "    for sent in X_sentences:\n",
    "        X_dict = {key: sent[i] for i,key in enumerate(feature_names)}\n",
    "        X_train.append(X_dict)\n",
    "        \n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'stack_pos_0'),\n",
       " (1, 'stack_word_0'),\n",
       " (2, 'queue_pos_0'),\n",
       " (3, 'queue_word_0'),\n",
       " (4, 'canRe'),\n",
       " (5, 'canLa')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(enumerate(train_features1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-e73396cd38ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX1_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_features1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX2_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_features2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX3_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_features3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-7b861ea76743>\u001b[0m in \u001b[0;36mfeature_dict\u001b[0;34m(X_sentences, feature_names)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_sentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mX_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-7b861ea76743>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_sentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mX_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "X1_dict = feature_dict(X_1, train_features1)\n",
    "X2_dict = feature_dict(X_2, train_features2)\n",
    "X3_dict = feature_dict(X_3, train_features3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **The parsing function, `parse_ml()`, takes the the `stack`, `queue`, `graph`, and the transition (`trans`) predicted by the classifier, and carries out the transition.**\n",
    "\n",
    "You can use this model and complete it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ml(stack,queue,graph,trans):\n",
    "    if stack and trans[:2] == 'ra':\n",
    "        stack, queue, graph = transition.right_arc(stack, queue, graph, trans[3:])\n",
    "        return stack, queue, graph, 'ra'\n",
    "    if stack and trans[:2] == 'la':\n",
    "        stack, queue, graph = transition.left_arc(stack, queue, graph, trans[3:])\n",
    "        return stack, queue, graph, 'la'\n",
    "    if stack and trans[:2] == 're':\n",
    "        stack, queue, graph = transition.reduce(stack, queue, graph)\n",
    "        return stack, queue, graph, 're'\n",
    "    stack, queue, graph = transition.shift(stack, queue, graph)\n",
    "    return stack, queue, graph, 'sh'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def parse_ml(stack, queue, graph, trans):\n",
    "    if stack and trans[:2] == 'ra':\n",
    "        stack, queue, graph = transition.right_arc(stack, queue, graph, trans[3:])\n",
    "        return stack, queue, graph, 'ra'\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where trans is either `ra.deprel`, `la.deprel`, `re`, or `sh`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **You will then use the partial `graph` to write the values of the `heads` and functions to the words.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model on features1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X1_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-935ad03a32a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX1_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X1_dict' is not defined"
     ]
    }
   ],
   "source": [
    "X1_dict[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stack_pos_0': 'AB',\n",
       " 'stack_word_0': 'mycket',\n",
       " 'queue_pos_0': 'AJ',\n",
       " 'queue_word_0': 'hetsigare',\n",
       " 'canRe': 'False',\n",
       " 'canLa': 'True'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_1[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X1_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-8a3b87962350>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX1_dict\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mX_1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X1_dict' is not defined"
     ]
    }
   ],
   "source": [
    "X1_dict == X_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding the features...\n"
     ]
    }
   ],
   "source": [
    "print(\"Encoding the features...\")\n",
    "# Vectorize the feature matrix and carry out a one-hot encoding\n",
    "vec = DictVectorizer(sparse=True)\n",
    "X = vec.fit_transform(X_1)\n",
    "# The statement below will swallow a considerable memory\n",
    "# X = vec.fit_transform(X_dict).toarray()\n",
    "# print(vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x40101 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 6 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model after extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexiscole/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training the model...\")\n",
    "classifier = linear_model.LogisticRegression(penalty='l2', dual=True, solver='liblinear')\n",
    "model = classifier.fit(X, y_1)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding the features...\n",
      "Training the model...\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Encoding the features...\")\n",
    "# Vectorize the feature matrix and carry out a one-hot encoding\n",
    "vec = DictVectorizer(sparse=True)\n",
    "X = vec.fit_transform(X_2)\n",
    "# The statement below will swallow a considerable memory\n",
    "# X = vec.fit_transform(X2_dict).toarray()\n",
    "# print(vec.get_feature_names())\n",
    "\n",
    "print(\"Training the model...\")\n",
    "classifier = linear_model.LogisticRegression(penalty='l2', dual=True, solver='liblinear')\n",
    "model_2 = classifier.fit(X, y_2)\n",
    "print(model_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### training the model on featureset 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding the features...\n",
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexiscole/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Encoding the features...\")\n",
    "# Vectorize the feature matrix and carry out a one-hot encoding\n",
    "vec = DictVectorizer(sparse=True)\n",
    "X = vec.fit_transform(X_3)\n",
    "# The statement below will swallow a considerable memory\n",
    "# X = vec.fit_transform(X3_dict).toarray()\n",
    "# print(vec.get_feature_names())\n",
    "\n",
    "print(\"Training the model...\")\n",
    "classifier = linear_model.LogisticRegression(penalty='l2', dual=True, solver='liblinear')\n",
    "model_3 = classifier.fit(X, y_3)\n",
    "print(model_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_test(sentences, feature_names, model):\n",
    "    \"\"\"\n",
    "    :param sentences:\n",
    "    :param w_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for sent in sentences:\n",
    "        stack = []\n",
    "        queue = list(sent)\n",
    "        graph = {}\n",
    "        graph['heads'] = {}\n",
    "        graph['heads']['0'] = '0'\n",
    "        graph['deprels'] = {}\n",
    "        graph['deprels']['0'] = 'ROOT'\n",
    "        \n",
    "        while queue:\n",
    "            features = extract(stack, queue, graph, feature_names, sent)\n",
    "            #features_dict = feature_dict([features], feature_names)\n",
    "            \n",
    "            # Vectorize the test sentence and one hot encoding\n",
    "            X_test = vec.transform(features)\n",
    "            #print(features)\n",
    "            #print(sent)\n",
    "            #print(X_test)\n",
    "            \n",
    "            # Predicts the chunks and returns numbers\n",
    "            trans_pred = classifier.predict(X_test)[0]\n",
    "            #print(trans_pred)\n",
    "\n",
    "            #trans_nr = classifier.predict()\n",
    "            \n",
    "            stack, queue, graph, trans = parse_ml(stack, queue, graph, trans_pred)\n",
    "        \n",
    "        stack, graph = dparser.transition.empty_stack(stack, graph)\n",
    "        \n",
    "        for i,word in enumerate(sent):\n",
    "            word['head'] = graph['heads'].get(str(i), str(0))\n",
    "            word['deprel'] = graph['deprels'].get(str(i), str(0))\n",
    "        \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Gold standard parser\n",
    "\"\"\"\n",
    "__author__ = \"Pierre Nugues\"\n",
    "\n",
    "column_names_2006 = ['id', 'form', 'lemma', 'cpostag', 'postag', 'feats', 'head', 'deprel', 'phead', 'pdeprel']\n",
    "column_names_2006_test = ['id', 'form', 'lemma', 'cpostag', 'postag', 'feats']\n",
    "    \n",
    "test_sentences = conll.read_sentences(\"swedish_talbanken05_test_blind.conll\")\n",
    "test_formatted_corpus = conll.split_rows(test_sentences, column_names_2006)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Finally, you will save the sentences in an output file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(file, formatted_corpus, column_names):\n",
    "    f_out = open(file, 'w')\n",
    "    for sentence in formatted_corpus:\n",
    "        for row in sentence[1:]:\n",
    "            # print(row, flush=True)\n",
    "            for col in column_names[:-1]:\n",
    "                if col in row:\n",
    "                    f_out.write(row[col] + '\\t')\n",
    "                else:\n",
    "                    f_out.write('_\\t')\n",
    "            col = column_names[-1]\n",
    "            if col in row:\n",
    "                f_out.write(row[col] + '\\n')\n",
    "            else:\n",
    "                f_out.write('_\\n')\n",
    "        f_out.write('\\n')\n",
    "    f_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Once you have parsed the test set, you will measure the accuracy of your parser using the CoNLL evaluation script [3] (where `-q` stands for quiet).**\n",
    "\n",
    "Local copy: [eval.pl]. You will run this script using the command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_test = extract_features_test(test_formatted_corpus, train_features1, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll.save('w1.conll', sentences_test, column_names_2006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Labeled   attachment score: 3471 / 5021 * 100 = 69.13 %\r\n",
      "  Unlabeled attachment score: 3826 / 5021 * 100 = 76.20 %\r\n",
      "  Label accuracy score:       3652 / 5021 * 100 = 72.73 %\r\n"
     ]
    }
   ],
   "source": [
    "!perl eval.pl -g swedish_talbanken05_test.conll -s w1.conll -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Labeled   attachment score: 3617 / 5021 * 100 = 72.04 %\r\n",
      "  Unlabeled attachment score: 3964 / 5021 * 100 = 78.95 %\r\n",
      "  Label accuracy score:       3797 / 5021 * 100 = 75.62 %\r\n"
     ]
    }
   ],
   "source": [
    "sentences_test_2 = extract_features_test(test_formatted_corpus, train_features2, model_2)\n",
    "conll.save('w2.conll', sentences_test_2, column_names_2006)\n",
    "!perl eval.pl -g swedish_talbanken05_test.conll -s w2.conll -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Labeled   attachment score: 3674 / 5021 * 100 = 73.17 %\r\n",
      "  Unlabeled attachment score: 4022 / 5021 * 100 = 80.10 %\r\n",
      "  Label accuracy score:       3894 / 5021 * 100 = 77.55 %\r\n"
     ]
    }
   ],
   "source": [
    "sentences_test_3 = extract_features_test(test_formatted_corpus, train_features3, model_3)\n",
    "conll.save('w3.conll', sentences_test_3, column_names_2006)\n",
    "!perl eval.pl -g swedish_talbanken05_test.conll -s w3.conll -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **You will run the parser with the three feature sets described in the fifth assignment to carry out a labelled dependency parsing.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **You need to reach a labelled attachment score of 75 to pass this lab.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraps from Lab 5.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Gold standard parser\n",
    "\"\"\"\n",
    "__author__ = \"Pierre Nugues\"\n",
    "\n",
    "column_names_2006 = ['id', 'form', 'lemma', 'cpostag', 'postag', 'feats', 'head', 'deprel', 'phead', 'pdeprel']\n",
    "column_names_2006_test = ['id', 'form', 'lemma', 'cpostag', 'postag', 'feats']\n",
    "    \n",
    "sentences = conll.read_sentences(train_file)\n",
    "formatted_corpus = conll.split_rows(sentences, column_names_2006)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def parse_ml(stack, queue, graph, trans):\n",
    "    if stack and trans[:2] == 'ra':\n",
    "        stack, queue, graph = transition.right_arc(stack, queue, graph, trans[3:])\n",
    "        return stack, queue, graph, 'ra'\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "_From Lab 5: dparser.py\n",
    "\"\"\"\n",
    "\n",
    "sent_cnt = 0\n",
    "\n",
    "for sentence in formatted_corpus:\n",
    "    sent_cnt += 1\n",
    "    if sent_cnt % 1000 == 0:\n",
    "        print(sent_cnt, 'sentences on', len(formatted_corpus), flush=True)\n",
    "    stack = []\n",
    "    queue = list(sentence)\n",
    "    graph = {}\n",
    "    graph['heads'] = {}\n",
    "    graph['heads']['0'] = '0'\n",
    "    graph['deprels'] = {}\n",
    "    graph['deprels']['0'] = 'ROOT'\n",
    "    transitions = []\n",
    "    \n",
    "    while queue:\n",
    "        # stack, queue, graph, trans = dparser.reference(stack, queue, graph)\n",
    "        # transitions.append(trans)\n",
    "        features.extract()\n",
    "        trans_nr = classifier.predict()\n",
    "    stack, queue, graph, trans = parse_ml(stack, queue, graph, trans)\n",
    "    \n",
    "    print('Equal graphs:', transition.equal_graphs(sentence, graph))\n",
    "    \n",
    "    # Poorman's projectivization to have well-formed graphs.\n",
    "    for word in sentence:\n",
    "        word['head'] = graph['heads'][word['id']]\n",
    "    # print(transitions)\n",
    "    # print(graph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
